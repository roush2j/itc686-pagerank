\begin{abstract} 
We attempted to implement the PageRank algorithm developed by Google.  We discuss
the theory and practice of the algorithm, provide a ready source for web crawl
data, and the \{dis\}advantages of Amazon cloud services.  We conclude that our
PageRank implementation is effective, but a full web ranking proved too technically
challenging.
\end{abstract}

\section{Introduction}
Web search engines must perform three major steps in order to produce meaningful
results:
\begin{enumerate}
\item Crawl the web, following hyperlinks to create a map of all reachable pages.
\item Build an index of the pages according to what (potential) search terms they contain.
\item When a query is submitted, identify and rank all of the pages containing the query terms.
\end{enumerate}
\define{PageRank} is a link analysis algorithm that assigns numerical weights 
to URLs in mutually linked set of documents, based on the number of links 
\emph{to} a document and the PageRank of the link sources.  These weights provide 
a starting point for ranking the pages on the web.
The underlying assumption is that more important websites are likely to receive 
more links from other websites - which turns out to match the reality
of the world wide web quite well.